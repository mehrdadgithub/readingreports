---
layout: post
comments: false
title: "Weisfeiler and Leman Go Neural: Higher-order Graph Neural Networks"
categories: misc
---

> Abstract: This work investigates GNNs from a theoretical point of view and relates them to the 1-WL, and shows that GNNs have the same expressiveness as 1-WL in terms of distinguishing non-isomorphic graphs. Hence, both algorithms also have the same shortcomings. To overcome this limitations, k-GNNs is proposed which take higher-order graph structures at multiple scales into account.

<!--more-->

{: class="table-of-content"}
* TOC
{:toc}

## Introduction
The WL-test generates node features through an iterative relabeling scheme:

- First, all nodes are assigned a common initial label; 
- Then, the algorithm iteratively relabels a node by aggregating over the multiset of label in its neighborhood
- The final feature representation of a graph is the histogram of the resulting node labels

Standard GNNs can be viewed as a neural version of 1-WL algorithm, where labels are replaced by continuous feature vectors and neural networks are used to aggregate over node neighborhoods.

This work shows that GNNs cannot be more powerful than the 1-WL in terms of distinguishing non-isomorphic graphs, e.g., the properties of subgraphs around each node. This result holds for a broad class of GNN architectures and all possible choices of parameters for them. 

Going further, the authors leverage the theoretical relationships between 1-WL and GNNs to propose a generalization of GNNs, called k-GNNs, which are neural architectures based on the k-WL, which are strictly more powerful than GNNs. The key insight in these higher-dimensional variants is that they perform message passing directly between subgraph structures, rather than individual nodes. This higher-order form of message passing can capture structural information that is not visible at the node-level.

## Preliminaries

### $1$-WL
Let $(G,l)$ be a labeled graph. In each iteration $t>0$, the $1$-WL computes a node labeling $c_l^{(t)}: V(G)\rightarrow \Sigma$ depends on the labeling from the previous iteration. 

In iteration $0$, we set $c_l^{(0)}=l$. For iteration $t>0$, we set 

$$
c_l^{(t)} = \text{hash}\big((c_l^{(t-1)}(v), \{c_l^{(t-1)}(u)|u \in N(v)\})\big)
$$

To test graph $G$ and $H$ for isomorphism, we run the above algorithm on both graphs.
If the number of labels between two iterations does not change, the algorithm terminates. Termination is guaranteed at most $\max\\{\|V(G)\|, \|V(H)\|\\}$ iterations.

### $k$-WL
The $k$-WL is a generalization of $1$-WL which colors tuples from $V(G)^k$ instead of nodes. We define the $j$-th neighborhood of a $k$-tuple $s=(s_1,...,s_k)$ in $V(G)^k$ as 

$$
N_j(s) = \{(s_1,...,s_{j-1},r,s_{j+1},...,s_k)|r \in V(G)\} 
$$

That is, the $j$-th neighborhood $N_j(t)$ of $s$ is obtained by replacing the $j$-th component of $s$ by every node from $V(G)$. 

In iteration $0$, we set each $k$-tuple as its atomic type.
For iteration $t>0$, we define

$$
C_j^{(t)}(s) = \text{hash}\big(\{c_{l,k}^{(t-1)}(s')|s' \in N_j(s)\}\big)
$$

and set

$$
c_{k,l}^{(t)}(s) = \text{hash}\big((c_{k,l}^{(t-1)}(s), \{C_1^{(t)}(s), ..., C_k^{(t)}(s)\})\big)
$$

### Graph Neural Networks
Let $(G,l)$ be a labeled graph with an initial node labeling $f^{(0)}: V(G)\rightarrow \mathbb{R}^{1\times d}$ that is consistent with $l$. 
A 1-GNN model can be implemented as 

$$
f^{(t)}(v) = f_\text{merge}^{W_1}\big(f^{(t-1)}(v),  f_\text{aggr}^{W_2}( \{f^{(t-1)}(w) | w \in N(v)\}) \big)
$$

### Shortcomings of 1-WL and 1-GNN
The power of 1-WL has been completely characterized, this characterization is also applicable to 1-GNNs. 

## $k$-dimensional Graph Neural Networks
Here we introduce a generalization of $1$-GNNs, which are based on the $k$-WL.

For a given $k$, we consider all $k$-element subsets $\[V(G)\]^k$ over $V(G)$. Let $s= \\{s_1,...,s_k\\}$ be a $k$-set in $\[V(G)\]^k$, then we define the neighborhood of $s$ as 

$$
N(s) = \{t \in |V(G)|^k : |s\cap t| = k-1\}
$$

The local neighborhood $N_L(s)$ consists of all $t\in N(s)$ such that $(v,w)\in E(G)$ for unique $v\in s\setminus t$ and the unique $w \in t \setminus s$. The global neighborhood $N_G(s)$ then is defined as $N(s) \setminus N_L(s)$. 

The set based $k$-WL works analogously to the $k$-WL. Initially, $c_{s,k,l}^{(0)}$ label each element $s$ in $\[V(G)\]^k$ with the isomorphism type of $G[s]$. Let $(G,l)$ be a labeled graph. In each $k$-GNN layer we compute a feature vector $f_k^{(t)}(s)$ for each $k$-set $s$ in $\[V(G)\]^k$ by

$$
f_k^{(t)}(s) = \sigma\big(f_k^{(t-1)}(s)\cdot W_1^{(t)} + \sum_{u \in N_L(s)} f_k^{(t-1)}(u) \cdot W_2^{(t)} \big)
$$


## Reference

