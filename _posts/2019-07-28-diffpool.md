---
layout: post
comments: false
title: "Hierarchical Graph Representation Learning with Differentiable Pooling"
categories: misc
---

> Abstract: Current GNN methods are inherently flat and do not learn hierarchical representations of graphs. In this work,  a differentiable graph pooling module, DIFFPOOL, is proposed that can generate hierarchical representations of graphs and can be combined with various graph neural network architectures in an end-to-end fashion.

<!--more-->

{: class="table-of-content"}
* TOC
{:toc}

## Introduction
A major limitation of current GNN architectures is that they are inherently flat as they only propagate information across the edges of the graph and are unable to infer and aggregate the information in a hierarchical way. This lack of hierarchical structure is especially problematic for the task of graph classification, where the goal is to predict the label associated with an entire graph.

The challenge in the GNN setting, compared to standard CNNs, is that graphs contain no natural notion of spatial locality, i.e., one cannot simply pool together all nodes in a $m \times m$ "patch" on a graph, because the complex topological structure of graphs precludes any straightforward, deterministic definition of a "patch". Moreover, unlike image data, graph data sets often contain graphs with varying numbers of nodes and edges, which makes defining a general graph pooling operator even more challenging.

To overcome the limitations, DIFFPOOL is proposed to learn a differentiable soft assignment at each layer of a deep GNN, mapping nodes to a set of clusters based on their learned embeddings.

![diffpool framework]({{'/assets/images/diffpool.png'|relative_url}})

## Method

### Preliminaries
We define a graph $G=(A,F)$ where $A\in\{0,1\}{n \times n}$ is the adjacent matrix and $F\in\mathbb{R}^{n \times d}$ is the node feature matrix assuming each node has $d$ features.

In this work, GNNs are used to learn the representations for graph classification in an end-to-end fashion:

$$
H^{(k)} = \text{MP}(A, H^{(k-1)}; \theta^{(k)}),
$$

where $H^{(k)} \in \mathbb{R}^{n \times d}$ are node embeddings computed after $k$ steps, and $\text{MP}(\cdot)$ is message passing function that depends on adjacent matrix $A$, trainable parameters $\theta^{(k)}$, and the node embeddings $H^{(k-1)}$. For simplicity, we use $Z = GNN(A, X )$ to denote an arbitrary GNN module implementing $K$ iterations of message passing $\text{MP}(\cdot)$.

The goal of this work is to learn how to cluster or pool together nodes using the output of a GNN. What makes designing such a pooling layer for GNNs especially challenging is that we need our model to learn a pooling strategy that will generalize across graphs with different nodes, edges, and that can adapt to the various graph structures during inference.

### Differentiable Pooling via Learned Assignments

The proposed DIFFPOOL addresses the above challenges by learning a cluster assignment matrix over the nodes using the output of a GNN model.

We denote the learned cluster assignment matrix at layer $l$ as $S^{(l)}\in \mathbb{R}^{n_l \times n_{l+1}}$. Each row of $S^{(l)}$ corresponds to one of the $n_{l+1}$ clusters at the next layer $l+1$. Intuitively, $S^{(l)}$ provides a soft assignment of each node at layer $l$ to a cluster in the next coarsened layer $l+1$.

We denote the input **adjacency matrix** at this layer as $A^{(l)}$ and denote the input **node embedding matrix** at this layer as $X^{(l)}$. Given these inputs, the DIFFPOOL layer

$$
(A^{(l+1)}, X^{(l+1)}) = \text{DIFFPOOL}(A^{(l)}, X^{(l)})
$$

coarsens the input graph, generating a new coarsened adjacency matrix $A^{(l+1)}$ and a new matrix of embeddings $X^{(l+1)}$ for each of the nodes in this coarsened graph.

In particular, we apply the two following equations in $\text{DIFFPOOL}(\cdot, \cdot)$:

$$
\begin{aligned}
Z^{(l)} &= \text{GNN}_{l, \text{embed}} (A^{(l)}, X^{(l)}) \\
S^{(l)} &= \text{Softmax}(\text{GNN}_{l, \text{pool}}(A^{(l)}, X^{(l)})) \\
X^{(l+1)} &= {S^{(l)}}^\top Z^{(l)} \in \mathbb{R}^{n_{l+1}\times d}\\
A^{(l+1)} &= {S^{(l)}}^\top A^{(l)} S^{(l)} \in \mathbb{R}^{n_{l+1}\times n_{l+1}}\\
L_{LP} &= \|A- S^{(l)} {S^{(l)}}^\top\|_2 \\
L_E &= \frac{1}{n} \sum_{i=1}^n S_i \log S_i
\end{aligned}
$$

Note that in order to be useful for graph classification, the pooling layer should be invariant under node permutations. For DIFFPOOL we get the following positive result, which shows that any deep GNN model based on DIFFPOOL is **permutation invariant**, as long as the component GNNs are permutation invariant.